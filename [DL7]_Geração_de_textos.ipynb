{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[DL7] Geração de textos.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tvaditya/intro_ds_and_ml/blob/main/%5BDL7%5D_Gera%C3%A7%C3%A3o_de_textos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOOQnK8J_e8U"
      },
      "source": [
        "# Projeto 16: Geração de textos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXVWhE70_y_e"
      },
      "source": [
        "Codificação adaptada da documentação do TensorFlow: https://www.tensorflow.org/beta/tutorials/text/text_generation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_TH1tpRimzF"
      },
      "source": [
        "- Usaremos uma base de dados extraída de textos escritos por Shakespeare's\n",
        "- Link: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "- O objetivo é treinar a LSTM para prever o próximo caractere em uma sequência de texto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0dvM1KcEHav"
      },
      "source": [
        "# Etapa 1: Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qAltyPMENLJ"
      },
      "source": [
        "# Etapa 2: Carregamento e exploração da base de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "source": [
        "data_url = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPsSlqlNOqYK"
      },
      "source": [
        "dataset_text = open(data_url, 'rb').read().decode(encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRALe5T8O5Jn"
      },
      "source": [
        "print(dataset_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqsecEFYO9IJ"
      },
      "source": [
        "len(dataset_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WneDt6rHPCOe"
      },
      "source": [
        "vocab = sorted(set(dataset_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZf6RMxyPL5j"
      },
      "source": [
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD8RitZ-PWhB"
      },
      "source": [
        "vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxMVlk8fEflX"
      },
      "source": [
        "# Etapa 3: Mapeamento de texto para números"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXkYldcQZe6R"
      },
      "source": [
        "char2idx = {char: index for index, char in enumerate(vocab)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV7VmtQzZ0iv"
      },
      "source": [
        "char2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G3i51saZ_wF"
      },
      "source": [
        "idx2char = np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhPJA-PDaEFG"
      },
      "source": [
        "idx2char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2owuCnGBaJat"
      },
      "source": [
        "idx2char[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koXJXy1WaMce"
      },
      "source": [
        "char2idx[':']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn9v9oqTaZEP"
      },
      "source": [
        "text_as_int = np.array([char2idx[char] for char in dataset_text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTNhYvMealiP"
      },
      "source": [
        "text_as_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp2BRhPUaz9I"
      },
      "source": [
        "text_as_int.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygklh9wHa40x"
      },
      "source": [
        "print('{} characters mapped to int ---> {}'.format(repr(dataset_text[:13]), text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1YVmnKNsXs0"
      },
      "source": [
        "# Etapa 4: Criação dos exemplos de treinamento e batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "- Dividiremos a base de dados em uma sequência de caracteres com \"seq_length\"\n",
        "- A saída (dados reais) será o mesmo que a entrada, porém, com um caractere deslocado\n",
        "- Exemplo com o texto \"Hello\" e seq_len = 4 \n",
        "    - Entrada: \"Hell\"\n",
        "    - Saída: \"ello\" \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgAONRxcchno"
      },
      "source": [
        "len(dataset_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3mrD9P2clGe"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(dataset_text) // seq_length\n",
        "examples_per_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZQACj38c9Mq"
      },
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcu7meMDdFMt"
      },
      "source": [
        "char_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3o9WOt0dbGJ"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrV1ydM7d1fN"
      },
      "source": [
        "sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1bkCKQid57Y"
      },
      "source": [
        "for item in sequences.take(50):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJQ2Si-xez89"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDjUnu9VfI-W"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSJx3ARufSIB"
      },
      "source": [
        "for input_example, target_example in dataset.take(10):\n",
        "  print('Input data:', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcNXA00Yf4GX"
      },
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhtFw0x3gCTd"
      },
      "source": [
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3_rVHwbgSR1"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jIdWG_C3DTl"
      },
      "source": [
        "# Etapa 5: Construção do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzmfanYdhAGe"
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGMqp79ehfKd"
      },
      "source": [
        "vocab_size = len(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW7UV9XLhijj"
      },
      "source": [
        "embedding_dim = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCfH6396hnQJ"
      },
      "source": [
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im7jRrkvh7AO"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "                               tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "                               tf.keras.layers.Dense(vocab_size)])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AssOTF3Xj2Pk"
      },
      "source": [
        "model = build_model(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtLC48wQkSs0"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(10):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ygqprhblLwM"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmJzqbv2lalo"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcsN3edRli_r"
      },
      "source": [
        "sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP3pz1_5luwE"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHct9-myl_0z"
      },
      "source": [
        "print('Input: \\n', repr(''.join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print('Next char predictions: \\n', repr(''.join(idx2char[sampled_indices])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "# Etapa 6: Treinamento do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Otimizador e loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxGxR4ESn0QH"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdvrx1PaoL1l"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsRggCl4ok7u"
      },
      "source": [
        "example_batch_loss.numpy().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEsW0pPUox3L"
      },
      "source": [
        "model.compile(optimizer='Adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkslnJq-o7h7"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execução do treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEhoBoipqhMi"
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(dataset, epochs = epochs, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "# Etapa 7: Geração de textos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "### Restauração do último checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvoJUGEKs_im"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_hHa-ULtPpZ"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz2OLsFvtuuS"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### Loop de previsão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K_AH8AauBze"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Número de caracteres a serem gerados\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Conversão dos caracteres iniciais de string para números\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Lista para armazenar os textos gerados pela rede neural\n",
        "  text_generated = []\n",
        "\n",
        "  # Parâmetro temperatura\n",
        "  # Valores baixos resultam em melhores textos (deve ser testado)\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Loop para gerar os textos\n",
        "  for i in range(num_generate):\n",
        "    # Previsões\n",
        "    predictions = model(input_eval)\n",
        "\n",
        "    # Tratamento das previsões\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # Passamos a previsão como próxima entrada da rede\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ineHdlNIx2IC"
      },
      "source": [
        "print(generate_text(model, start_string='ROMEO: '))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}